2025-04-29 12:24:55,897 INFO - Task context logging is enabled
2025-04-29 12:24:55,901 INFO - Loaded executor: SequentialExecutor
2025-04-29 12:24:55,993 INFO - Starting the scheduler
2025-04-29 12:24:55,996 INFO - Processing each file at most -1 times
2025-04-29 12:24:56,008 INFO - Launched DagFileProcessorManager with pid: 5149
2025-04-29 12:24:56,012 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 12:24:56,022 INFO - Configured default timezone UTC
2025-04-29 12:26:32,747 INFO - 1 tasks up for execution:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:26:31.024362+00:00 [scheduled]>
2025-04-29 12:26:32,747 INFO - DAG sqlite_to_csv_dag has 0/16 running and queued tasks
2025-04-29 12:26:32,748 INFO - Setting the following tasks to queued state:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:26:31.024362+00:00 [scheduled]>
2025-04-29 12:26:32,777 WARNING - cannot record scheduled_duration for task read_from_sqlite_and_write_to_csv because previous state change time has not been saved
2025-04-29 12:26:32,780 INFO - Sending TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:26:31.024362+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2025-04-29 12:26:32,782 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:26:31.024362+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:26:32,818 INFO - Executing command: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:26:31.024362+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:26:42,401 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:26:31.024362+00:00', try_number=1, map_index=-1)
2025-04-29 12:26:42,446 INFO - TaskInstance Finished: dag_id=sqlite_to_csv_dag, task_id=read_from_sqlite_and_write_to_csv, run_id=manual__2025-04-29T09:26:31.024362+00:00, map_index=-1, run_start_date=2025-04-29 09:26:40.496017+00:00, run_end_date=2025-04-29 09:26:41.086283+00:00, run_duration=0.590266, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-29 09:26:32.749888+00:00, queued_by_job_id=3, pid=5264
2025-04-29 12:27:10,423 INFO - 1 tasks up for execution:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:26:31.024362+00:00 [scheduled]>
2025-04-29 12:27:10,424 INFO - DAG sqlite_to_csv_dag has 0/16 running and queued tasks
2025-04-29 12:27:10,426 INFO - Setting the following tasks to queued state:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:26:31.024362+00:00 [scheduled]>
2025-04-29 12:27:10,429 INFO - Sending TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:26:31.024362+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default
2025-04-29 12:27:10,430 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:26:31.024362+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:27:10,448 INFO - Executing command: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:26:31.024362+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:27:17,718 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:26:31.024362+00:00', try_number=2, map_index=-1)
2025-04-29 12:27:17,732 INFO - TaskInstance Finished: dag_id=sqlite_to_csv_dag, task_id=read_from_sqlite_and_write_to_csv, run_id=manual__2025-04-29T09:26:31.024362+00:00, map_index=-1, run_start_date=2025-04-29 09:26:40.496017+00:00, run_end_date=2025-04-29 09:26:41.086283+00:00, run_duration=0.590266, state=success, executor_state=success, try_number=2, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-29 09:27:10.427686+00:00, queued_by_job_id=3, pid=5264
2025-04-29 12:27:17,979 INFO - Marking run <DagRun sqlite_to_csv_dag @ 2025-04-29 09:26:31.024362+00:00: manual__2025-04-29T09:26:31.024362+00:00, state:running, queued_at: 2025-04-29 09:26:31.126898+00:00. externally triggered: True> successful
2025-04-29 12:27:17,981 INFO - DagRun Finished: dag_id=sqlite_to_csv_dag, execution_date=2025-04-29 09:26:31.024362+00:00, run_id=manual__2025-04-29T09:26:31.024362+00:00, run_start_date=2025-04-29 09:26:32.531317+00:00, run_end_date=2025-04-29 09:27:17.981197+00:00, run_duration=45.44988, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-29 09:26:31.024362+00:00, data_interval_end=2025-04-29 09:26:31.024362+00:00, dag_hash=ed0b396909932cff6a4dea690f874053
2025-04-29 12:27:31,676 INFO - 1 tasks up for execution:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:27:30.975254+00:00 [scheduled]>
2025-04-29 12:27:31,677 INFO - DAG sqlite_to_csv_dag has 0/16 running and queued tasks
2025-04-29 12:27:31,677 INFO - Setting the following tasks to queued state:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:27:30.975254+00:00 [scheduled]>
2025-04-29 12:27:31,681 WARNING - cannot record scheduled_duration for task read_from_sqlite_and_write_to_csv because previous state change time has not been saved
2025-04-29 12:27:31,682 INFO - Sending TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:27:30.975254+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2025-04-29 12:27:31,682 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:27:30.975254+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:27:31,697 INFO - Executing command: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:27:30.975254+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:27:43,262 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:27:30.975254+00:00', try_number=1, map_index=-1)
2025-04-29 12:27:43,270 INFO - TaskInstance Finished: dag_id=sqlite_to_csv_dag, task_id=read_from_sqlite_and_write_to_csv, run_id=manual__2025-04-29T09:27:30.975254+00:00, map_index=-1, run_start_date=2025-04-29 09:27:40.974910+00:00, run_end_date=2025-04-29 09:27:41.816222+00:00, run_duration=0.841312, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-29 09:27:31.678933+00:00, queued_by_job_id=3, pid=5314
2025-04-29 12:28:21,911 INFO - 1 tasks up for execution:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:27:30.975254+00:00 [scheduled]>
2025-04-29 12:28:21,912 INFO - DAG sqlite_to_csv_dag has 0/16 running and queued tasks
2025-04-29 12:28:21,913 INFO - Setting the following tasks to queued state:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:27:30.975254+00:00 [scheduled]>
2025-04-29 12:28:21,931 INFO - Sending TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:27:30.975254+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default
2025-04-29 12:28:21,932 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:27:30.975254+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:28:21,950 INFO - Executing command: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:27:30.975254+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:28:31,257 ERROR - Failed to execute task Command '['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:27:30.975254+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']' returned non-zero exit status 1..
2025-04-29 12:28:31,259 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:27:30.975254+00:00', try_number=2, map_index=-1)
2025-04-29 12:28:31,294 INFO - TaskInstance Finished: dag_id=sqlite_to_csv_dag, task_id=read_from_sqlite_and_write_to_csv, run_id=manual__2025-04-29T09:27:30.975254+00:00, map_index=-1, run_start_date=2025-04-29 09:27:40.974910+00:00, run_end_date=2025-04-29 09:27:41.816222+00:00, run_duration=0.841312, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-29 09:28:21.929112+00:00, queued_by_job_id=3, pid=5314
2025-04-29 12:28:31,294 ERROR - Executor reports task instance <TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:27:30.975254+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
2025-04-29 12:28:31,314 ERROR - Executor reports task instance <TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:27:30.975254+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
2025-04-29 12:28:31,356 INFO - Marking task as UP_FOR_RETRY. dag_id=sqlite_to_csv_dag, task_id=read_from_sqlite_and_write_to_csv, execution_date=20250429T092730, start_date=20250429T092740, end_date=20250429T092831
2025-04-29 12:29:57,104 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 12:30:39,147 INFO - Marking run <DagRun sqlite_to_csv_dag @ 2025-04-29 09:27:30.975254+00:00: manual__2025-04-29T09:27:30.975254+00:00, state:running, queued_at: 2025-04-29 09:27:31.010063+00:00. externally triggered: True> successful
2025-04-29 12:30:39,148 INFO - DagRun Finished: dag_id=sqlite_to_csv_dag, execution_date=2025-04-29 09:27:30.975254+00:00, run_id=manual__2025-04-29T09:27:30.975254+00:00, run_start_date=2025-04-29 09:27:31.591928+00:00, run_end_date=2025-04-29 09:30:39.148103+00:00, run_duration=187.556175, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-29 09:27:30.975254+00:00, data_interval_end=2025-04-29 09:27:30.975254+00:00, dag_hash=cc2794bc72486abb0dcc8f059d9f07c4
2025-04-29 12:30:43,160 INFO - 1 tasks up for execution:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:30:42.717829+00:00 [scheduled]>
2025-04-29 12:30:43,161 INFO - DAG sqlite_to_csv_dag has 0/16 running and queued tasks
2025-04-29 12:30:43,161 INFO - Setting the following tasks to queued state:
	<TaskInstance: sqlite_to_csv_dag.read_from_sqlite_and_write_to_csv manual__2025-04-29T09:30:42.717829+00:00 [scheduled]>
2025-04-29 12:30:43,166 WARNING - cannot record scheduled_duration for task read_from_sqlite_and_write_to_csv because previous state change time has not been saved
2025-04-29 12:30:43,169 INFO - Sending TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:30:42.717829+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2025-04-29 12:30:43,173 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:30:42.717829+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:30:43,231 INFO - Executing command: ['airflow', 'tasks', 'run', 'sqlite_to_csv_dag', 'read_from_sqlite_and_write_to_csv', 'manual__2025-04-29T09:30:42.717829+00:00', '--local', '--subdir', 'DAGS_FOLDER/sample_dag.py']
2025-04-29 12:30:51,854 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='sqlite_to_csv_dag', task_id='read_from_sqlite_and_write_to_csv', run_id='manual__2025-04-29T09:30:42.717829+00:00', try_number=1, map_index=-1)
2025-04-29 12:30:51,873 INFO - TaskInstance Finished: dag_id=sqlite_to_csv_dag, task_id=read_from_sqlite_and_write_to_csv, run_id=manual__2025-04-29T09:30:42.717829+00:00, map_index=-1, run_start_date=2025-04-29 09:30:49.795637+00:00, run_end_date=2025-04-29 09:30:50.325296+00:00, run_duration=0.529659, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-29 09:30:43.164104+00:00, queued_by_job_id=3, pid=5485
2025-04-29 12:30:52,623 INFO - Marking run <DagRun sqlite_to_csv_dag @ 2025-04-29 09:30:42.717829+00:00: manual__2025-04-29T09:30:42.717829+00:00, state:running, queued_at: 2025-04-29 09:30:42.750212+00:00. externally triggered: True> successful
2025-04-29 12:30:52,624 INFO - DagRun Finished: dag_id=sqlite_to_csv_dag, execution_date=2025-04-29 09:30:42.717829+00:00, run_id=manual__2025-04-29T09:30:42.717829+00:00, run_start_date=2025-04-29 09:30:43.080800+00:00, run_end_date=2025-04-29 09:30:52.624159+00:00, run_duration=9.543359, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-29 09:30:42.717829+00:00, data_interval_end=2025-04-29 09:30:42.717829+00:00, dag_hash=cc2794bc72486abb0dcc8f059d9f07c4
2025-04-29 12:34:57,952 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 12:39:58,354 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 12:44:58,391 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 12:49:58,593 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 12:54:59,135 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:00:00,369 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:05:01,696 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:10:01,883 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:15:02,089 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:20:02,510 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:25:02,563 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:30:02,650 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:35:02,948 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:40:03,131 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:45:03,431 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:50:03,644 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:55:04,132 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-29 13:58:49,994 ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/kiwilytics/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/kiwilytics/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: database or disk is full

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/kiwilytics/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 202, in heartbeat
    job2025-05-05 16:36:59,433 INFO - Task context logging is enabled
2025-05-05 16:36:59,440 INFO - Loaded executor: SequentialExecutor
2025-05-05 16:36:59,621 INFO - Starting the scheduler
2025-05-05 16:36:59,625 INFO - Processing each file at most -1 times
2025-05-05 16:36:59,650 INFO - Launched DagFileProcessorManager with pid: 6192
2025-05-05 16:36:59,654 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-05-05 16:36:59,686 INFO - Configured default timezone UTC
2025-05-05 16:37:00,185 INFO - Marked 1 SchedulerJob instances as failed
2025-05-05 16:42:00,338 INFO - Adopting or resetting orphaned tasks for active dag runs
